{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import yaml\n",
    "import time\n",
    "from random import shuffle\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from collections import Counter\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "pd.set_option('display.max_columns', 200)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_pandas_df_from_json(path):\n",
    "    '''\n",
    "    INPUT: filepath string\n",
    "    OUTPUT: pandas database\n",
    "    '''\n",
    "    return pd.read_json(file_path, lines=True)\n",
    "\n",
    "def is_food(item):\n",
    "    '''\n",
    "    INPUT: cell from pandas dataframe\n",
    "    OUTPUT: boolean\n",
    "    '''\n",
    "    restaurants_and_related_categories = ['Restaurants', 'Italian','Food', 'Bars','Fast Food', 'Coffee & Tea', 'Sandwiches']\n",
    "    if len(set(restaurants_and_related_categories) & set(item)) >= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def flatten_dict(row):\n",
    "    out = {}\n",
    "    for key, value in row.items():\n",
    "        if type(value) != dict:\n",
    "            out[key] = value\n",
    "        else:\n",
    "            sub_key = key\n",
    "            for k, v in value.items():\n",
    "                out[sub_key + \"|\" + k] = v\n",
    "    return out\n",
    "\n",
    "def make_exists_function(key):\n",
    "    def get_key_if_exists(row):\n",
    "        if key in row:\n",
    "            return row[key]\n",
    "        else:\n",
    "            return \"N/A\"\n",
    "    return get_key_if_exists\n",
    "\n",
    "def add_restaurant_count_column(dataframe):\n",
    "    restaurant_frequency = dataframe.groupby(['name']).count().sort_values('address', ascending=False)\n",
    "\n",
    "    restaurant_frequency = pd.DataFrame(restaurant_frequency['address'])\n",
    "\n",
    "    restaurant_frequency.columns = ['restaurant_count']\n",
    "\n",
    "    restaurant_frequency['name'] = restaurant_frequency.index\n",
    "\n",
    "    restaurant_frequency = restaurant_frequency[['name', 'restaurant_count']]\n",
    "\n",
    "    return previously_open_US_restaurants.merge(restaurant_frequency, how='left', left_on='name', right_on='name')\n",
    "\n",
    "def closed_on_google(row):\n",
    "    try:\n",
    "        return row[0]['permanently_closed']\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def fix_percent(row):\n",
    "    row = str(row).strip('%')\n",
    "    row = float(row)\n",
    "    return row/100\n",
    "\n",
    "def summaries_from_google(dataframe, key, default_val=0):\n",
    "    summaries = []\n",
    "    key_errors = 0\n",
    "    for i in range(len(dataframe)):\n",
    "        total = 0\n",
    "        count = 0\n",
    "        for j in range(len(dataframe['results'][i])):\n",
    "            try:\n",
    "                total += dataframe['results'][i][j][key]\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "                key_errors += 1\n",
    "        try:\n",
    "            summaries.append({'business_id': nearby_df['yelp_business_id'][i], 'avg_'+key: (total / count)})\n",
    "        except ZeroDivisionError:\n",
    "            summaries.append({'business_id': nearby_df['yelp_business_id'][i], 'avg_'+key: default_val})\n",
    "    return pd.DataFrame(summaries)\n",
    "\n",
    "def get_price(row):\n",
    "    try:\n",
    "        return row['RestaurantsPriceRange2']\n",
    "    except KeyError:\n",
    "        return 1.5\n",
    "    \n",
    "def concat_unique_columns(df1, df2, suffix):\n",
    "    cols = list(set(list(df1.columns) + list(df2.columns)))\n",
    "    df_dict = {'df1':[], 'df2':[]}\n",
    "    for col in cols:\n",
    "        if col in df1.columns:\n",
    "            df_dict['df1'].append(col)\n",
    "        else:\n",
    "            df_dict['df2'].append(col)\n",
    "    combined_df = pd.concat([df1[df_dict['df1']],df2[df_dict['df2']]],axis=1)\n",
    "    combined_df.columns = [suffix + str(col) for col in combined_df.columns]\n",
    "    return combined_df\n",
    "\n",
    "def get_zipped_postcode_data_from_s3_bucket(postcodes):\n",
    "    s3 = boto3.client('s3')\n",
    "    zip_code_data = []\n",
    "    for code in postcodes:\n",
    "        response = s3.get_object(Bucket='zip-code-economic-data', Key=f'zip_code: {code}')\n",
    "        body = response['Body'].read()\n",
    "        df = pd.read_html(body)[0][pd.read_html(body)[0]['Measure'].map(type) == str][['Description', 'Measure']]\n",
    "        keys = [str(x) for x in list(df['Description'].values)]\n",
    "        vals = [str(x) for x in list(df['Measure'].values)]\n",
    "        zipped = dict(zip(keys, vals))\n",
    "        zipped['Zip Code'] = code\n",
    "        zip_code_data.append(zipped)\n",
    "    return zip_code_data\n",
    "\n",
    "def str_to_num(row):\n",
    "    try:\n",
    "        return int(row)\n",
    "    except ValueError:\n",
    "        return float(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'https://s3-us-west-2.amazonaws.com/businesspredictiondata/business.json'\n",
    "yelp_business_data = create_pandas_df_from_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Cutting down yelp data to only open US restaurants...\")\n",
    "\n",
    "#filters businesses that were open when this dataset was published Jan. 2018\n",
    "open_businesses = yelp_business_data[yelp_business_data['is_open'] == 1].drop_duplicates(['name','address'])\n",
    "\n",
    "#creates column that says if business is restaurant and creates df of just open restaurants\n",
    "open_businesses['is_food'] = open_businesses['categories'].apply(is_food)\n",
    "open_restaurants = open_businesses[open_businesses['is_food'] == True].copy()\n",
    "\n",
    "#creates column that says if business is in USA and creates df of just\n",
    "#restaurants open in the US as of January 2018\n",
    "\n",
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\",\n",
    "      \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\",\n",
    "      \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n",
    "      \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\n",
    "      \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "open_restaurants['in_US'] = open_restaurants['state'].isin(states)\n",
    "mask = (open_restaurants['in_US'] == True) & (open_restaurants['longitude'] < -20)\n",
    "previously_open_US_restaurants = open_restaurants.loc[mask].copy()\n",
    "\n",
    "\n",
    "print(\"Creating dummy columns for restaurant attributes...\")\n",
    "#creates dummy columns for 0\n",
    "previously_open_US_restaurants['flat_attributes'] = previously_open_US_restaurants.loc[:,'attributes'].apply(flatten_dict)\n",
    "all_attributes = []\n",
    "\n",
    "for row in previously_open_US_restaurants['flat_attributes']:\n",
    "    all_attributes.extend(row.keys())\n",
    "unique_attributes = list(dict(Counter(all_attributes).most_common(50)).keys())\n",
    "\n",
    "for key in unique_attributes:\n",
    "    f = make_exists_function(key)\n",
    "    previously_open_US_restaurants['Attribute|' +key + ' value:'] = previously_open_US_restaurants['flat_attributes'].apply(f)\n",
    "    \n",
    "all_categories = []\n",
    "[all_categories.extend(item) for item in list(previously_open_US_restaurants['categories'])]\n",
    "\n",
    "most_common_categories = list(dict(Counter(all_categories).most_common(50)).keys())\n",
    "\n",
    "for key in most_common_categories:\n",
    "    previously_open_US_restaurants[f\"Category|{key}_true\"] = previously_open_US_restaurants['categories'].apply(lambda x: key in x)\n",
    "\n",
    "previously_open_US_restaurants = add_restaurant_count_column(previously_open_US_restaurants)\n",
    "\n",
    "previously_open_US_restaurants['restaurant_count > 1'] = previously_open_US_restaurants['restaurant_count'] > 1\n",
    "previously_open_US_restaurants['restaurant_count > 5'] = previously_open_US_restaurants['restaurant_count'] > 5\n",
    "previously_open_US_restaurants['restaurant_count > 25'] = previously_open_US_restaurants['restaurant_count'] > 25\n",
    "\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "restaurants = client['restaurants']\n",
    "google_places = restaurants['google_places']\n",
    "start_time = time.time()\n",
    "\n",
    "google_df = pd.DataFrame(list(google_places.find()))\n",
    "\n",
    "google_df = google_df[['queried_name', 'yelp_business_id', 'results']]\n",
    "\n",
    "google_df['closed_on_google'] = google_df['results'].apply(closed_on_google)\n",
    "\n",
    "restaurants_with_google_data = previously_open_US_restaurants.merge(google_df, how='inner', left_on='business_id', right_on='yelp_business_id')\n",
    "\n",
    "#removes rows without any matching data from Google\n",
    "restaurants_with_google_data = restaurants_with_google_data[restaurants_with_google_data['results'].map(len) > 0]\n",
    "\n",
    "#gets the valid postal codes from the dataframe.\n",
    "postcodes = list(previously_open_US_restaurants['postal_code'].unique())\n",
    "\n",
    "postcodes = [x for x in postcodes if len(x) > 2]\n",
    "\n",
    "#grabs the zip code data from the s3 bucket and turns it into a dataframe\n",
    "zip_code_dicts = get_zipped_postcode_data_from_s3_bucket(postcodes)\n",
    "\n",
    "zip_code_df = pd.DataFrame(zip_code_dicts)\n",
    "\n",
    "zip_code_df['Zip Code'] = zip_code_df['Zip Code'].apply(str)\n",
    "\n",
    "restaurants_with_economic_data = restaurants_with_google_data.merge(zip_code_df, how='left', left_on='postal_code', right_on='Zip Code')\n",
    "\n",
    "restaurants_with_economic_data.iloc[:,-19:] = restaurants_with_economic_data.iloc[:,-19:].fillna(0).copy()\n",
    "\n",
    "percent_columns = ['Educational Attainment: Percent high school graduate or higher', 'Individuals below poverty level']\n",
    "for col in percent_columns:\n",
    "    restaurants_with_economic_data[col] = restaurants_with_economic_data[col].apply(fix_percent)\n",
    "\n",
    "num_columns = ['2016 ACS 5-Year Population Estimate',\n",
    " 'American Indian and Alaska Native alone',\n",
    " 'Asian alone',\n",
    " 'Black or African American alone',\n",
    " 'Census 2010 Total Population',\n",
    " 'Foreign Born Population',\n",
    " 'Hispanic or Latino (of any race)',\n",
    " 'Median Age',\n",
    " 'Median Household Income',\n",
    " 'Native Hawaiian and Other Pacific Islander alone',\n",
    " 'Some Other Race alone',\n",
    " 'Total housing units',\n",
    " 'Two or More Races',\n",
    " 'Veterans',\n",
    " 'White alone']\n",
    "    \n",
    "\n",
    "for col in num_columns:\n",
    "    restaurants_with_economic_data[col] = restaurants_with_economic_data[col].apply(str_to_num)\n",
    "\n",
    "#adds nearby data using google maps api data: among nearby restaurants: 1) count 2) avg_rating 3) avg_price\n",
    "maps_nearby = restaurants['maps_nearby']\n",
    "nearby_df = pd.DataFrame(list(maps_nearby.find()))\n",
    "nearby_df['num_nearby_restaurants'] = nearby_df['results'].apply(lambda x: len(x))\n",
    "\n",
    "nearby_prices = summaries_from_google(nearby_df, 'price_level', 1.5)\n",
    "nearby_ratings = summaries_from_google(nearby_df, 'rating', 3.5)\n",
    "nearby_prices_and_rating = nearby_prices.merge(nearby_ratings, how='outer', on='business_id')\n",
    "nearby_prices_rating_num = nearby_prices_and_rating.merge(nearby_df, how='outer', left_on='business_id', right_on='yelp_business_id')\n",
    "trimmed_nearby_data = nearby_prices_rating_num[['business_id','avg_price_level','avg_rating','num_nearby_restaurants']]\n",
    "\n",
    "restaurants_with_nearby_data = restaurants_with_economic_data.merge(trimmed_nearby_data, how='left', on='business_id')\n",
    "\n",
    "restaurants_with_nearby_data['relative rating'] = restaurants_with_nearby_data['stars'] - restaurants_with_nearby_data['avg_rating']\n",
    "\n",
    "restaurants_with_nearby_data['price_level'] = restaurants_with_nearby_data['attributes'].apply(get_price)\n",
    "\n",
    "restaurants_with_nearby_data['relative_price'] = restaurants_with_nearby_data['price_level'] - restaurants_with_nearby_data['avg_price_level']\n",
    "\n",
    "for col in num_columns:\n",
    "    restaurants_with_economic_data[col] = restaurants_with_economic_data[col].apply(str_to_num)\n",
    "\n",
    "#adds nearby data using google maps api data: among nearby restaurants: 1) count 2) avg_rating 3) avg_price\n",
    "maps_nearby = restaurants['maps_nearby']\n",
    "nearby_df = pd.DataFrame(list(maps_nearby.find()))\n",
    "nearby_df['num_nearby_restaurants'] = nearby_df['results'].apply(lambda x: len(x))\n",
    "\n",
    "nearby_prices = summaries_from_google(nearby_df, 'price_level', 1.5)\n",
    "nearby_ratings = summaries_from_google(nearby_df, 'rating', 3.5)\n",
    "nearby_prices_and_rating = nearby_prices.merge(nearby_ratings, how='outer', on='business_id')\n",
    "nearby_prices_rating_num = nearby_prices_and_rating.merge(nearby_df, how='outer', left_on='business_id', right_on='yelp_business_id')\n",
    "trimmed_nearby_data = nearby_prices_rating_num[['business_id','avg_price_level','avg_rating','num_nearby_restaurants']]\n",
    "\n",
    "restaurants_with_nearby_data = restaurants_with_economic_data.merge(trimmed_nearby_data, how='left', on='business_id')\n",
    "\n",
    "restaurants_with_nearby_data['relative rating'] = restaurants_with_nearby_data['stars'] - restaurants_with_nearby_data['avg_rating']\n",
    "\n",
    "restaurants_with_nearby_data['price_level'] = restaurants_with_nearby_data['attributes'].apply(get_price)\n",
    "\n",
    "restaurants_with_nearby_data['relative_price'] = restaurants_with_nearby_data['price_level'] - restaurants_with_nearby_data['avg_price_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-16f6da3e3738>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://s3-us-west-2.amazonaws.com/businesspredictiondata/review.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreview_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_pandas_df_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-9323d2966987>\u001b[0m in \u001b[0;36mcreate_pandas_df_from_json\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mOUTPUT\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     '''\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_food\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/json/json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     )\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gzip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1007\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Reading review data from s3 bucket...\")\n",
    "file_path = 'https://s3-us-west-2.amazonaws.com/businesspredictiondata/review.json'\n",
    "review_data = create_pandas_df_from_json(file_path)n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "with open('../data/review.json') as f:\n",
    "    for line in f:\n",
    "        reviews.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-7b9f748e00f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreviews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   6285\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6286\u001b[0m         return _list_of_dict_to_arrays(data, columns,\n\u001b[0;32m-> 6287\u001b[0;31m                                        coerce_float=coerce_float, dtype=dtype)\n\u001b[0m\u001b[1;32m   6288\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6289\u001b[0m         return _list_of_series_to_arrays(data, columns,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_list_of_dict_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m   6406\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6408\u001b[0;31m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdicts_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6409\u001b[0m     return _convert_object_array(content, columns, dtype=dtype,\n\u001b[1;32m   6410\u001b[0m                                  coerce_float=coerce_float)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reviews_df = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e68b8b4e3daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/review.json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mreviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reviews_df = pd.DataFrame(reviews)\n",
    "\n",
    "five_star_reviews = reviews_df[reviews_df['stars'] == 5]\n",
    "two_to_four_star_reviews = reviews_df[reviews_df['stars'].isin([2,3,4])]\n",
    "one_star_reviews = reviews_df[reviews_df['stars'] == 1]\n",
    "\n",
    "review_series = [five_star_reviews, two_to_four_star_reviews, one_star_reviews]\n",
    "\n",
    "for i in range(len(review_series)):\n",
    "    review_series[i] = review_series[i].groupby('business_id')['text'].apply(lambda x: \"{%s}\" % ':::'.join(x))\n",
    "    review_series[i] = pd.DataFrame(review_series[i])\n",
    "    review_series[i]['business_id'] = review_series[i].index\n",
    "\n",
    "restaurants_with_stars = restaurants_with_nearby_data.merge(review_series[0], how='left', on='business_id')\n",
    "restaurants_with_stars = restaurants_with_stars.rename({'text': 'five_star_review_text'}, axis='columns')\n",
    "restaurants_with_stars = restaurants_with_stars.merge(review_series[1], how='left', on='business_id')\n",
    "restaurants_with_stars = restaurants_with_stars.rename({'text': 'two_to_four_star_review_text'}, axis='columns')\n",
    "restaurants_with_stars = restaurants_with_stars.merge(review_series[2], how='left', on='business_id')\n",
    "restaurants_with_stars = restaurants_with_stars.rename({'text': 'one_star_review_text'}, axis='columns')\n",
    "\n",
    "restaurants_with_stars[['five_star_review_text', 'two_to_four_star_review_text', 'one_star_review_text']] = restaurants_with_stars[['five_star_review_text', 'two_to_four_star_review_text', 'one_star_review_text']].fillna(\"Empty\")\n",
    "\n",
    "closed_restaurants = restaurants_with_stars[restaurants_with_stars['closed_on_google'] == True]\n",
    "open_restaurants = restaurants_with_stars[restaurants_with_stars['closed_on_google'] == False]\n",
    "\n",
    "tfidf_five_closed = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "tfidf_five_closed.fit(closed_restaurants['five_star_review_text'])\n",
    "feature_matrix = tfidf_five_closed.transform(restaurants_with_stars['one_star_review_text'])\n",
    "tfidf_five_closed_df = pd.DataFrame(feature_matrix.toarray(), columns=tfidf_five_closed.get_feature_names())\n",
    "\n",
    "tfidf_five_open = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "tfidf_five_open.fit(open_restaurants['five_star_review_text'])\n",
    "feature_matrix = tfidf_five_open.transform(restaurants_with_stars['five_star_review_text'])\n",
    "tfidf_five_open_df = pd.DataFrame(feature_matrix.toarray(), columns=tfidf_five_open.get_feature_names())\n",
    "\n",
    "tfidf_two_to_four_closed = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "tfidf_two_to_four_closed.fit(closed_restaurants['two_to_four_star_review_text'])\n",
    "feature_matrix = tfidf_two_to_four_closed.transform(restaurants_with_stars['one_star_review_text'])\n",
    "tfidf_two_to_four_closed_df = pd.DataFrame(feature_matrix.toarray(), columns=tfidf_two_to_four_closed.get_feature_names())\n",
    "\n",
    "tfidf_two_to_four_open = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "tfidf_two_to_four_open.fit(open_restaurants['two_to_four_star_review_text'])\n",
    "feature_matrix = tfidf_two_to_four_open.transform(restaurants_with_stars['two_to_four_star_review_text'])\n",
    "tfidf_two_to_four_open_df = pd.DataFrame(feature_matrix.toarray(), columns=tfidf_two_to_four_open.get_feature_names())\n",
    "\n",
    "tfidf_one_closed = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "tfidf_one_closed.fit(closed_restaurants['one_star_review_text'])\n",
    "feature_matrix = tfidf_one_closed.transform(restaurants_with_stars['one_star_review_text'])\n",
    "tfidf_one_closed_df = pd.DataFrame(feature_matrix.toarray(), columns=tfidf_one_closed.get_feature_names())\n",
    "\n",
    "tfidf_one_open = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "tfidf_one_open.fit(open_restaurants['one_star_review_text'])\n",
    "feature_matrix = tfidf_one_open.transform(restaurants_with_stars['one_star_review_text'])\n",
    "tfidf_one_open_df = pd.DataFrame(feature_matrix.toarray(), columns=tfidf_one_open.get_feature_names())\n",
    "\n",
    "unique_one_star_df = concat_unique_columns(tfidf_one_closed_df, tfidf_one_open_df, 'one_star: ')\n",
    "unique_two_to_four_star_df = concat_unique_columns(tfidf_two_to_four_closed_df, tfidf_two_to_four_open_df, '2-4_star: ')\n",
    "unique_five_star_df = concat_unique_columns(tfidf_five_closed_df, tfidf_five_open_df, 'five-star: ')\n",
    "\n",
    "all_tfidf_reviews_df = pd.concat([unique_one_star_df, unique_two_to_four_star_df, unique_five_star_df], axis=1)\n",
    "\n",
    "restaurants_with_reviews = pd.concat([restaurants_with_stars,all_tfidf_reviews_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restaurants_with_reviews = restaurants_with_reviews.drop_duplicates(['name','address'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restaurants_with_reviews.to_csv('../data/featurized_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
